services:
  ollama:
    image: ollama/ollama:latest
    container_name: ine-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      OLLAMA_HOST: "0.0.0.0"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  ine-relay:
    build:
      context: .
      dockerfile: Dockerfile.relay
    container_name: ine-relay-bot
    network_mode: host
    env_file:
      - .env
    environment:
      OLLAMA_URL: "http://127.0.0.1:11434"
      OLLAMA_MODEL: "bazobehram/qwen3-14b-claude-4.5-opus-high-reasoning"
      RELAY_TIMEOUT_MS: "120000"
      HISTORY_MAX_MESSAGES: "5"
      # ComfyUI integration
      COMFYUI_URL: "http://127.0.0.1:8188"
    depends_on:
      - ollama
    restart: unless-stopped

  # ComfyUI - Local image generation (CPU mode)
  comfyui:
    image: ardenius/comfyui-cpu:latest
    container_name: ine-comfyui
    ports:
      - "8188:8188"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - comfyui-data:/workspace/ComfyUI
      - comfyui-models:/workspace/ComfyUI/models
    command: python3 /ComfyUI-cpu/main.py --listen 0.0.0.0 --cpu --cpu-vae
    environment:
      COMFYUI_PORT: "8188"
    restart: unless-stopped

volumes:
  ollama-data:
  comfyui-data:
  comfyui-models:
